{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43e4a9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karvsmech/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from datasets import load_dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "896baf3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4192f8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate(example):\n",
    "    return {\n",
    "        'text': \" \".join(example['text'].split()[:50]),\n",
    "        'label': example['label']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3314343f",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_dataset = load_dataset('imdb')\n",
    "small_imdb_dataset = DatasetDict(\n",
    "    train=imdb_dataset['train'].shuffle(seed=1111).select(range(128)).map(truncate),\n",
    "    val=imdb_dataset['train'].shuffle(seed=1111).select(range(128, 160)).map(truncate)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cccbd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': [\"Probably Jackie Chan's best film in the 1980s, and the one that put him on the map. The scale of this self-directed police drama is evident from the opening and closing scenes, during which a squatters' village and shopping mall are demolished. There are, clearly, differences between the original Chinese\", 'A wonderful movie! Anyone growing up in an Italian family will definitely see themselves in these characters. A good family movie with sadness, humor, and very good acting from all. You will enjoy this movie!! We need more like it.', 'HORRENDOUS! Avoid like the plague. I would rate this in the top 10 worst movies ever. Special effects, acting, mood, sound, etc. appear to be done by day care students...wait, I have seen programs better than this. Opens like a soft porn show with a blurred nude female doing a', 'And I absolutely adore Isabelle Blais!!! She was so cute in this movie, and far different from her role in \"Quebec-Montreal\" where she was more like a man-eater. I think she should have been nominated for a Jutra. I mean, Syvlie Moreau was good, but Isabelle was far superior, IMO.', 'Must confess to having seen a few howlers in my time, but this one is up there with the worst of them. Plot troubling to follow. Sex and violence thrown in to disorient and distract from the really poorly put together film.<br /><br />I can only imagine that the cast', \"I pity people calling kamal hassan 'ulaganaayakan' maybe for them ulagam is tollywood ! comeon guys..this movie is a thriller without thrill..<br /><br />come out of your ulagam and just watch some high class thrillers like The Usual Suspects or even The Silence of the Lambs.<br /><br />technically good but\", 'I remember my parents not understanding Saturday Night Live when I was 15. They also did not understand Rock n Roll and many other things. Now that I am approaching their age, I still remember, and find I understand many of the things my kids love. But this is pathetic.', 'This animated movie is a masterpiece! The narration, music, animation, and storyline where all remarkable. My girlfriend and I saw it again for a second time and we got more insight from it. We invited a couple friends to see Spirit with us and they really enjoyed it a lot.', 'I vaugely recall seeing this when I was 3 years old, then my parents accidentally taped over all but a few seconds of it with some other cartoon. Then I was about 8 or 9 years old when I rediscovered it and since I was then able to comprehend things', 'I, like many people, saw this film in the theatre when it first came out in \\'97. It was a below average film at best, defiantly not the \"masterpiece\" that all these \"Titanic\" fanboys like to make it out as. First off, DiCaprio is a terrible actor no matter which'], 'label': [1, 1, 0, 1, 0, 0, 0, 1, 1, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(small_imdb_dataset['train'][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64adec85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 128\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 32\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint =  'siebert/sentiment-roberta-large-english'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# prepares the dataset - this tokenizes the dataset in batches of 16 examples.\n",
    "small_tokenized_dataset = small_imdb_dataset.map(lambda x: tokenizer(x['text'], padding=True, truncation=True),\n",
    "                       batched=True,\n",
    "                       batch_size=16\n",
    "                      )\n",
    "\n",
    "small_tokenized_dataset = small_tokenized_dataset.remove_columns(['text'])\n",
    "small_tokenized_dataset = small_tokenized_dataset.rename_column(\"label\", 'labels')\n",
    "small_tokenized_dataset.set_format('torch')\n",
    "\n",
    "small_tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aef62bac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': tensor([1, 1]),\n",
       " 'input_ids': tensor([[    0, 35882, 12585,  8710,    18,   275,   822,    11,     5,  5114,\n",
       "             29,     6,     8,     5,    65,    14,   342,   123,    15,     5,\n",
       "           5456,     4,    20,  3189,     9,    42,  1403,    12, 25706,   249,\n",
       "           4149,    16, 10180,    31,     5,  1273,     8,  3172,  5422,     6,\n",
       "            148,    61,    10, 31147,  2696,   108,  3375,     8,  3482,  9367,\n",
       "             32, 20766,     4,   345,    32,     6,  2563,     6,  5550,   227,\n",
       "              5,  1461,  1111,     2,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1],\n",
       "         [    0,   250,  4613,  1569,   328,  6142,  1197,    62,    11,    41,\n",
       "           3108,   284,    40,  2299,   192,  1235,    11,   209,  3768,     4,\n",
       "             83,   205,   284,  1569,    19, 17437,     6, 12073,     6,     8,\n",
       "            182,   205,  3501,    31,    70,     4,   370,    40,  2254,    42,\n",
       "           1569, 12846,   166,   240,    55,   101,    24,     4,     2,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_tokenized_dataset['train'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ac58d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(small_tokenized_dataset['train'], batch_size=16)\n",
    "eval_dataloader = DataLoader(small_tokenized_dataset['val'], batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abc986f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9866512",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1a59de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe72c66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b498831",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfda4695",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d0d153",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec156c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00056367",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc36a34f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa2890e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8759bf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptorch",
   "language": "python",
   "name": "ptorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
