{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad89a47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5c6480e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "table.dataframe td, table.dataframe th {\n",
       "    border: 1px  black solid !important;\n",
       "  color: black !important;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<style type=\"text/css\">\n",
    "table.dataframe td, table.dataframe th {\n",
    "    border: 1px  black solid !important;\n",
    "  color: black !important;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e487e480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karvsmech/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea4a093b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_encoding(model_inputs, indent=4):\n",
    "    indent_str = \" \" * indent\n",
    "    print(\"{\")\n",
    "    for k, v in model_inputs.items():\n",
    "        print(indent_str + k + \":\")\n",
    "        print(indent_str + indent_str + str(v))\n",
    "    print(\"}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a35f0025",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('siebert/sentiment-roberta-large-english')\n",
    "# initialize the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained('siebert/sentiment-roberta-large-english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba3e2f7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,   100,   437,  2283,     7,  1532,    59, 30581,  3923, 12346,\n",
       "         34379,   328,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-3.7605,  2.9262]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the prediction is: POSITIVE\n"
     ]
    }
   ],
   "source": [
    "inputs = \"I'm excited to learn about Hugging Face Transformers!\"\n",
    "tokenized_inputs = tokenizer(inputs, return_tensors='pt')\n",
    "tokenized_inputs\n",
    "\n",
    "outputs = model(**tokenized_inputs)\n",
    "outputs\n",
    "\n",
    "labels = ['NEGATIVE', 'POSITIVE']\n",
    "prediction = labels[torch.argmax(outputs.logits)]\n",
    "\n",
    "print(f'the prediction is: {prediction}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ab5a2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertTokenizer(name_or_path='distilbert-base-cased', vocab_size=28996, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)\n",
      "DistilBertTokenizerFast(name_or_path='distilbert-base-cased', vocab_size=28996, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)\n",
      "DistilBertTokenizerFast(name_or_path='distilbert-base-cased', vocab_size=28996, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)\n"
     ]
    }
   ],
   "source": [
    "# tokenizers\n",
    "\n",
    "from transformers import DistilBertTokenizer, DistilBertTokenizerFast, AutoTokenizer\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')\n",
    "print(tokenizer)\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased')\n",
    "print(tokenizer)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-cased')\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8006cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 20164, 10932, 10289, 11303, 1468, 1110, 1632, 106, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "input_str = \"Hugging Face transformers is great!\"\n",
    "tokenized_inputs = tokenizer(input_str)\n",
    "\n",
    "print(tokenized_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ff4f9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    input_ids:\n",
      "        [101, 20164, 10932, 10289, 11303, 1468, 1110, 1632, 106, 102]\n",
      "    attention_mask:\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print_encoding(tokenized_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "becafe78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start:                    Hugging Face transformers is great!\n",
      "tokenize:                 ['Hu', '##gging', 'Face', 'transform', '##ers', 'is', 'great', '!']\n",
      "convert_tokens_to_ids:    [20164, 10932, 10289, 11303, 1468, 1110, 1632, 106]\n",
      "add special tokens:       [101, 20164, 10932, 10289, 11303, 1468, 1110, 1632, 106, 102]\n",
      "---------\n",
      "decode:                   [CLS] Hugging Face transformers is great! [SEP]\n"
     ]
    }
   ],
   "source": [
    "cls = [tokenizer.cls_token_id]\n",
    "sep = [tokenizer.sep_token_id]\n",
    "\n",
    "# tokenization happens in a few steps\n",
    "input_tokens = tokenizer.tokenize(input_str)\n",
    "input_ids = tokenizer.convert_tokens_to_ids(input_tokens)\n",
    "input_ids_special_tokens = cls + input_ids + sep\n",
    "\n",
    "decoded_str = tokenizer.decode(input_ids_special_tokens)\n",
    "\n",
    "\n",
    "print(\"start:                   \", input_str)\n",
    "print(\"tokenize:                \", input_tokens)\n",
    "print(\"convert_tokens_to_ids:   \", input_ids)\n",
    "print(\"add special tokens:      \", input_ids_special_tokens)\n",
    "print(\"---------\")\n",
    "print(\"decode:                  \", decoded_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "566f2531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face transformers is great!\n",
      "-----\n",
      "Number of tokens: 10\n",
      "Ids: [101, 20164, 10932, 10289, 11303, 1468, 1110, 1632, 106, 102]\n",
      "Tokens: ['[CLS]', 'Hu', '##gging', 'Face', 'transform', '##ers', 'is', 'great', '!', '[SEP]']\n",
      "special_tokens_mask: [1, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "# for fast tokenizers there is another option too\n",
    "inputs = tokenizer._tokenizer.encode(input_str)\n",
    "\n",
    "print(input_str)\n",
    "print(\"-\" * 5)\n",
    "print(f\"Number of tokens: {len(inputs)}\")\n",
    "print(f\"Ids: {inputs.ids}\")\n",
    "print(f\"Tokens: {inputs.tokens}\")\n",
    "print(f\"special_tokens_mask: {inputs.special_tokens_mask}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2839eaa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Tensors:\n",
      "{\n",
      "    input_ids:\n",
      "        tensor([[  101, 20164, 10932, 10289, 11303,  1468,  1110,  1632,   106,   102]])\n",
      "    attention_mask:\n",
      "        tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# the tokenizer can return pytorch tensors\n",
    "model_inputs = tokenizer(input_str, return_tensors='pt')\n",
    "print('PyTorch Tensors:')\n",
    "print_encoding(model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "535ca348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad token: [PAD] | Pad token id: 0\n",
      "Padding:\n",
      "{\n",
      "    input_ids:\n",
      "        tensor([[  101, 20164, 10932, 10289, 13809, 14467, 19134,  1110,  1632,   106,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1109,  3613,  3058, 17594, 15457,  1166,  1103, 16688,  3676,\n",
      "           119,  1599,  1103,  3676,  1400,  1146,  1105,  1868,  1283,  1272,\n",
      "          1131,  1238,   112,   189,  1176, 17594,  1279,   119,   102]])\n",
      "    attention_mask:\n",
      "        tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1]])\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# we can pass multiple strings to the tokenizer and pad them as we need.\n",
    "model_inputs = tokenizer([\"Hugging Face Transfomers is great!\",\n",
    "                         \"The quick brown fox jumps over the lazy dog. Then the dog got up and ran away because she didn't like foxes.\",\n",
    "                         ],\n",
    "                        return_tensors='pt', \n",
    "                        padding=True, \n",
    "                        truncation=True)\n",
    "print(f\"Pad token: {tokenizer.pad_token} | Pad token id: {tokenizer.pad_token_id}\")\n",
    "print(\"Padding:\")\n",
    "print_encoding(model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a4cecb3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS] Hugging Face Transfomers is great! [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
       " \"[CLS] The quick brown fox jumps over the lazy dog. Then the dog got up and ran away because she didn't like foxes. [SEP]\"]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(model_inputs.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "35d1f573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hugging Face Transfomers is great!',\n",
       " \"The quick brown fox jumps over the lazy dog. Then the dog got up and ran away because she didn't like foxes.\"]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(model_inputs.input_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7bf200",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptorch",
   "language": "python",
   "name": "ptorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
