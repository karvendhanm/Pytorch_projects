{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ef4dd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72d24ca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "table.dataframe td, table.dataframe th {\n",
       "    border: 1px  black solid !important;\n",
       "  color: black !important;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<style type=\"text/css\">\n",
    "table.dataframe td, table.dataframe th {\n",
    "    border: 1px  black solid !important;\n",
    "  color: black !important;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d66c7ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karvsmech/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a2091e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_encoding(model_inputs, indent=4):\n",
    "    indent_str = \" \" * indent\n",
    "    print(\"{\")\n",
    "    for k, v in model_inputs.items():\n",
    "        print(indent_str + k + \":\")\n",
    "        print(indent_str + indent_str + str(v))\n",
    "    print(\"}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88adf741",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('siebert/sentiment-roberta-large-english')\n",
    "# initialize the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained('siebert/sentiment-roberta-large-english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c365eaa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,   100,   437,  2283,     7,  1532,    59, 30581,  3923, 12346,\n",
       "         34379,   328,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-3.7605,  2.9262]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the prediction is: POSITIVE\n"
     ]
    }
   ],
   "source": [
    "inputs = \"I'm excited to learn about Hugging Face Transformers!\"\n",
    "tokenized_inputs = tokenizer(inputs, return_tensors='pt')\n",
    "tokenized_inputs\n",
    "\n",
    "outputs = model(**tokenized_inputs)\n",
    "outputs\n",
    "\n",
    "labels = ['NEGATIVE', 'POSITIVE']\n",
    "prediction = labels[torch.argmax(outputs.logits)]\n",
    "\n",
    "print(f'the prediction is: {prediction}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6d9b411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertTokenizer(name_or_path='distilbert-base-cased', vocab_size=28996, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)\n",
      "DistilBertTokenizerFast(name_or_path='distilbert-base-cased', vocab_size=28996, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)\n",
      "DistilBertTokenizerFast(name_or_path='distilbert-base-cased', vocab_size=28996, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)\n"
     ]
    }
   ],
   "source": [
    "# tokenizers\n",
    "\n",
    "from transformers import DistilBertTokenizer, DistilBertTokenizerFast, AutoTokenizer\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')\n",
    "print(tokenizer)\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased')\n",
    "print(tokenizer)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-cased')\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd8e1065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 20164, 10932, 10289, 11303, 1468, 1110, 1632, 106, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "input_str = \"Hugging Face transformers is great!\"\n",
    "tokenized_inputs = tokenizer(input_str)\n",
    "\n",
    "print(tokenized_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48f7360c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    input_ids:\n",
      "        [101, 20164, 10932, 10289, 11303, 1468, 1110, 1632, 106, 102]\n",
      "    attention_mask:\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print_encoding(tokenized_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56979ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start:                    Hugging Face transformers is great!\n",
      "tokenize:                 ['Hu', '##gging', 'Face', 'transform', '##ers', 'is', 'great', '!']\n",
      "convert_tokens_to_ids:    [20164, 10932, 10289, 11303, 1468, 1110, 1632, 106]\n",
      "add special tokens:       [101, 20164, 10932, 10289, 11303, 1468, 1110, 1632, 106, 102]\n",
      "---------\n",
      "decode:                   [CLS] Hugging Face transformers is great! [SEP]\n"
     ]
    }
   ],
   "source": [
    "cls = [tokenizer.cls_token_id]\n",
    "sep = [tokenizer.sep_token_id]\n",
    "\n",
    "# tokenization happens in a few steps\n",
    "input_tokens = tokenizer.tokenize(input_str)\n",
    "input_ids = tokenizer.convert_tokens_to_ids(input_tokens)\n",
    "input_ids_special_tokens = cls + input_ids + sep\n",
    "\n",
    "decoded_str = tokenizer.decode(input_ids_special_tokens)\n",
    "\n",
    "\n",
    "print(\"start:                   \", input_str)\n",
    "print(\"tokenize:                \", input_tokens)\n",
    "print(\"convert_tokens_to_ids:   \", input_ids)\n",
    "print(\"add special tokens:      \", input_ids_special_tokens)\n",
    "print(\"---------\")\n",
    "print(\"decode:                  \", decoded_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1bbcb2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face transformers is great!\n",
      "-----\n",
      "Number of tokens: 10\n",
      "Ids: [101, 20164, 10932, 10289, 11303, 1468, 1110, 1632, 106, 102]\n",
      "Tokens: ['[CLS]', 'Hu', '##gging', 'Face', 'transform', '##ers', 'is', 'great', '!', '[SEP]']\n",
      "special_tokens_mask: [1, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "# for fast tokenizers there is another option too\n",
    "inputs = tokenizer._tokenizer.encode(input_str)\n",
    "\n",
    "print(input_str)\n",
    "print(\"-\" * 5)\n",
    "print(f\"Number of tokens: {len(inputs)}\")\n",
    "print(f\"Ids: {inputs.ids}\")\n",
    "print(f\"Tokens: {inputs.tokens}\")\n",
    "print(f\"special_tokens_mask: {inputs.special_tokens_mask}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2b96a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Tensors:\n",
      "{\n",
      "    input_ids:\n",
      "        tensor([[  101, 20164, 10932, 10289, 11303,  1468,  1110,  1632,   106,   102]])\n",
      "    attention_mask:\n",
      "        tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# the tokenizer can return pytorch tensors\n",
    "model_inputs = tokenizer(input_str, return_tensors='pt')\n",
    "print('PyTorch Tensors:')\n",
    "print_encoding(model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb7fa172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad token: [PAD] | Pad token id: 0\n",
      "Padding:\n",
      "{\n",
      "    input_ids:\n",
      "        tensor([[  101, 20164, 10932, 10289, 13809, 14467, 19134,  1110,  1632,   106,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1109,  3613,  3058, 17594, 15457,  1166,  1103, 16688,  3676,\n",
      "           119,  1599,  1103,  3676,  1400,  1146,  1105,  1868,  1283,  1272,\n",
      "          1131,  1238,   112,   189,  1176, 17594,  1279,   119,   102]])\n",
      "    attention_mask:\n",
      "        tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1]])\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# we can pass multiple strings to the tokenizer and pad them as we need.\n",
    "model_inputs = tokenizer([\"Hugging Face Transfomers is great!\",\n",
    "                         \"The quick brown fox jumps over the lazy dog. Then the dog got up and ran away because she didn't like foxes.\",\n",
    "                         ],\n",
    "                        return_tensors='pt', \n",
    "                        padding=True, \n",
    "                        truncation=True)\n",
    "print(f\"Pad token: {tokenizer.pad_token} | Pad token id: {tokenizer.pad_token_id}\")\n",
    "print(\"Padding:\")\n",
    "print_encoding(model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1af1750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS] Hugging Face Transfomers is great! [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
       " \"[CLS] The quick brown fox jumps over the lazy dog. Then the dog got up and ran away because she didn't like foxes. [SEP]\"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(model_inputs.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7977956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hugging Face Transfomers is great!',\n",
       " \"The quick brown fox jumps over the lazy dog. Then the dog got up and ran away because she didn't like foxes.\"]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(model_inputs.input_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21b71de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ceda9ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at siebert/sentiment-roberta-large-english were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('siebert/sentiment-roberta-large-english')\n",
    "# initialize the model\n",
    "# when you need the model for just getting its representations.\n",
    "model_rep = AutoModel.from_pretrained('siebert/sentiment-roberta-large-english')\n",
    "\n",
    "# when you need the model with a particular head. \n",
    "# For instance, in this case we have sequence classification head for sentiment analysis.\n",
    "model_head = AutoModelForSequenceClassification.from_pretrained('siebert/sentiment-roberta-large-english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9614fc23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,   100,   437,  2283,     7,  1532,    59, 30581,  3923, 12346,\n",
       "         34379,   328,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.0525, -0.1342, -0.5488,  ...,  0.1036,  0.0879, -0.2249],\n",
       "         [-0.0568, -0.1349, -0.5527,  ...,  0.1023,  0.0883, -0.2277],\n",
       "         [-0.0568, -0.1349, -0.5525,  ...,  0.1026,  0.0884, -0.2279],\n",
       "         ...,\n",
       "         [-0.0576, -0.1346, -0.5536,  ...,  0.1007,  0.0885, -0.2269],\n",
       "         [-0.0565, -0.1349, -0.5526,  ...,  0.1028,  0.0883, -0.2282],\n",
       "         [-0.0578, -0.1352, -0.5541,  ...,  0.1014,  0.0881, -0.2279]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 0.1733, -0.9172,  0.3356,  ...,  0.5361, -0.4033, -0.8523]],\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-3.7605,  2.9262]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = \"I'm excited to learn about Hugging Face Transformers!\"\n",
    "model_inputs = tokenizer(inputs, return_tensors=\"pt\")\n",
    "model_inputs\n",
    "\n",
    "outputs_representation = model_rep(**model_inputs)\n",
    "outputs_representation\n",
    "\n",
    "outputs_sequence_classification_head = model_head(**model_inputs)\n",
    "outputs_sequence_classification_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "761de109",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1103, 2523, 1108, 1363,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.0588, -0.0790]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution over labels: tensor([[0.5344, 0.4656]], grad_fn=<SoftmaxBackward0>)\n",
      "NEGATIVE\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, DistilBertForSequenceClassification\n",
    "\n",
    "check_point = 'distilbert-base-cased'\n",
    "model_auto = AutoModelForSequenceClassification.from_pretrained(check_point)\n",
    "# model_bert = DistilBertForSequenceClassification.from_pretrained(check_point, num_labels = 2)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(check_point)\n",
    "\n",
    "input_str = \"the movie was good\"\n",
    "tokenized_inputs = tokenizer(input_str, return_tensors='pt')\n",
    "tokenized_inputs\n",
    "\n",
    "model_outputs = model_auto(**tokenized_inputs)\n",
    "model_outputs\n",
    "\n",
    "print(f\"Distribution over labels: {torch.softmax(model_outputs.logits, dim=1)}\")\n",
    "\n",
    "labels = ['NEGATIVE', 'POSITIVE']\n",
    "print(labels[torch.argmax(model_outputs.logits)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f58ab4c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   0,  627, 1569,   21,  205,    2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-3.6781,  2.8360]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution over labels: tensor([[0.0015, 0.9985]], grad_fn=<SoftmaxBackward0>)\n",
      "POSITIVE\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, DistilBertForSequenceClassification\n",
    "\n",
    "check_point = 'siebert/sentiment-roberta-large-english'\n",
    "model_auto = AutoModelForSequenceClassification.from_pretrained(check_point)\n",
    "# model_bert = DistilBertForSequenceClassification.from_pretrained(check_point, num_labels = 2)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(check_point)\n",
    "\n",
    "input_str = \"the movie was good\"\n",
    "tokenized_inputs = tokenizer(input_str, return_tensors='pt')\n",
    "tokenized_inputs\n",
    "\n",
    "model_outputs = model_auto(**tokenized_inputs)\n",
    "model_outputs\n",
    "\n",
    "print(f\"Distribution over labels: {torch.softmax(model_outputs.logits, dim=1)}\")\n",
    "\n",
    "labels = ['NEGATIVE', 'POSITIVE']\n",
    "print(labels[torch.argmax(model_outputs.logits)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34aeefa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptorch",
   "language": "python",
   "name": "ptorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
